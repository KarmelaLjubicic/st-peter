{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import operator, functools, time, itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, recall_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib, time, zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.546098</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.068420</td>\n",
       "      <td>-0.479087</td>\n",
       "      <td>-0.445000</td>\n",
       "      <td>3.442480</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.546098</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.248837</td>\n",
       "      <td>0.481288</td>\n",
       "      <td>1.866526</td>\n",
       "      <td>2.286476</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.546098</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.164975</td>\n",
       "      <td>0.481288</td>\n",
       "      <td>1.866526</td>\n",
       "      <td>2.286476</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.546098</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009230</td>\n",
       "      <td>0.481288</td>\n",
       "      <td>1.866526</td>\n",
       "      <td>2.286476</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.546098</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.379021</td>\n",
       "      <td>0.481288</td>\n",
       "      <td>1.866526</td>\n",
       "      <td>2.286476</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pclass  survived       age     sibsp     parch      fare  sex_female  \\\n",
       "0 -1.546098         1 -0.068420 -0.479087 -0.445000  3.442480           1   \n",
       "1 -1.546098         1 -2.248837  0.481288  1.866526  2.286476           0   \n",
       "2 -1.546098         0 -2.164975  0.481288  1.866526  2.286476           1   \n",
       "3 -1.546098         0  0.009230  0.481288  1.866526  2.286476           0   \n",
       "4 -1.546098         0 -0.379021  0.481288  1.866526  2.286476           1   \n",
       "\n",
       "   sex_male  embarked_C  embarked_Q  embarked_S  \n",
       "0         0           0           0           1  \n",
       "1         1           0           0           1  \n",
       "2         0           0           0           1  \n",
       "3         1           0           0           1  \n",
       "4         0           0           0           1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def titanic_dataset():\n",
    "    dataset = pd.read_csv(\"data\\\\Titanic.csv\")\n",
    "    del dataset[\"name\"]\n",
    "    del dataset[\"cabin\"]\n",
    "    del dataset[\"ticket\"]\n",
    "    del dataset[\"home.dest\"]\n",
    "    del dataset[\"body\"]\n",
    "    del dataset[\"boat\"]\n",
    "    \n",
    "    for column in [\"age\", \"sibsp\", \"parch\", \"fare\"]:\n",
    "        imp = Imputer()\n",
    "        dataset[column] = imp.fit_transform(dataset[column].values.reshape(-1,1))\n",
    "    \n",
    "    dataset[\"sibsp\"] =  dataset[\"sibsp\"].astype(float)\n",
    "    dataset = pd.get_dummies(dataset, columns=[\"sex\", \"embarked\"], prefix=[\"sex\", \"embarked\"])\n",
    "    \n",
    "    need_scaling = [\"pclass\",\"age\", \"sibsp\", \"parch\", \"fare\"]\n",
    "    scaler = StandardScaler()\n",
    "    for feature in need_scaling:\n",
    "        dataset[feature] = scaler.fit_transform(dataset[feature].values.reshape(-1,1))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = titanic_dataset()\n",
    "dataset.head()\n",
    "#print(\"Dataset shape:\", dataset.shape)\n",
    "#print(\"Survived rate:\\n\", dataset['survived'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset_good shape: (500, 10)\n",
      "Dataset_bad shape: (809, 10)\n",
      "Survived rate: 0.3819709702062643\n",
      "Deceased rate: 0.6180290297937356\n"
     ]
    }
   ],
   "source": [
    "dataset_good = dataset[dataset['survived'] == 1]\n",
    "dataset_bad = dataset[dataset['survived'] == 0]\n",
    "del dataset_good['survived']\n",
    "del dataset_bad['survived']\n",
    "\n",
    "print(\"Dataset_good shape:\", dataset_good.shape)\n",
    "print(\"Dataset_bad shape:\", dataset_bad.shape)\n",
    "\n",
    "good_rate = 500/1309\n",
    "bad_rate = 809/1309\n",
    "print(\"Survived rate:\", good_rate)\n",
    "print(\"Deceased rate:\", bad_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratificirana podjela - provjera koliko je potrebno \"dobrih\" i \"loših\" primjera u train i test setu. Sampliranje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of survived for train: 400\n",
      "Number of deceased for train: 647\n",
      "Number of survived for test: 100\n",
      "Number of deceased for test: 162\n",
      "\n",
      "Dimensions for symmetric classifier:\n",
      "(400, 10) (100, 10) (647, 10) (162, 10)\n",
      "\n",
      "Dimensions for base classifier:\n",
      "(1047, 10) (1047, 1) (262, 10) (262, 1)\n"
     ]
    }
   ],
   "source": [
    "train_size = 1309 * 0.8\n",
    "test_size = 1309 * 0.2\n",
    "print(\"Number of survived for train:\", round(train_size*good_rate))\n",
    "print(\"Number of deceased for train:\", round(train_size*bad_rate))\n",
    "print(\"Number of survived for test:\", round(test_size*good_rate))\n",
    "print(\"Number of deceased for test:\", round(test_size*bad_rate))\n",
    "\n",
    "good_train, good_test = train_test_split(dataset_good, test_size=0.20)\n",
    "bad_train, bad_test = train_test_split(dataset_bad, test_size=0.20)\n",
    "print(\"\\nDimensions for symmetric classifier:\")\n",
    "print(good_train.shape, good_test.shape, bad_train.shape, bad_test.shape)\n",
    "\n",
    "y_train = np.array([1]*len(good_train) + [0]*len(bad_train)).reshape(-1,1)\n",
    "x_train = good_train.append(bad_train)\n",
    "x_train['survived'] = y_train\n",
    "x_train = shuffle(x_train)\n",
    "y_train = x_train['survived'].reshape(-1,1)\n",
    "del x_train['survived']\n",
    "\n",
    "y_test = np.array([1]*len(good_test) + [0]*len(bad_test)).reshape(-1,1)\n",
    "x_test = good_test.append(bad_test)\n",
    "x_test['survived'] = y_test\n",
    "x_test = shuffle(x_test)\n",
    "y_test = x_test['survived'].reshape(-1,1)\n",
    "del x_test['survived']\n",
    "\n",
    "print(\"\\nDimensions for base classifier:\")\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_network(input_placeholder, keep_prob):\n",
    "    #reg = tf.contrib.layers.l2_regularizer(scale=0.001)\n",
    "    #layer_1 = tf.layers.dense(input_placeholder, 70, tf.nn.relu, name=\"layer_1\", reuse=tf.AUTO_REUSE,\n",
    "                             #kernel_regularizer = reg)\n",
    "    #drop_out1 = tf.nn.dropout(layer_1, keep_prob)\n",
    "    #layer_2 = tf.layers.dense(layer_1, 60, tf.nn.relu, name=\"layer_2\", reuse=tf.AUTO_REUSE,\n",
    "                             #kernel_regularizer = reg)\n",
    "    #drop_out2 = tf.nn.dropout(layer_2, keep_prob)\n",
    "    #layer_3 = tf.layers.dense(input_placeholder, 40, tf.nn.relu, name=\"layer_3\", reuse=tf.AUTO_REUSE)\n",
    "                             #kernel_regularizer = reg)\n",
    "    #drop_out3 = tf.nn.dropout(layer_3, keep_prob)\n",
    "    layer_4 = tf.layers.dense(input_placeholder, 10, tf.nn.relu, name=\"layer_4\", reuse=tf.AUTO_REUSE)\n",
    "                             #kernel_regularizer = reg)\n",
    "    #drop_out4 = tf.nn.dropout(layer_4, keep_prob)\n",
    "    layer_5 = tf.layers.dense(layer_4, 5, tf.nn.relu, name=\"layer_5\", reuse=tf.AUTO_REUSE)\n",
    "                             #kernel_regularizer = reg)\n",
    "    #drop_out5 = tf.nn.dropout(layer_5, keep_prob)\n",
    "    output = tf.layers.dense(layer_5, 1, name=\"output\", reuse=tf.AUTO_REUSE)\n",
    "                             #kernel_regularizer = reg)\n",
    "    return output\n",
    "\n",
    "def somers_d_score(y_true, y_score, base):\n",
    "    print(y_score.count(0.5))\n",
    "    if base:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        print('Somers\\' D scoreeee = ', 2 * auc_score - 1)\n",
    "    #print(y_score)\n",
    "    else:\n",
    "        for o in y_score:\n",
    "            if o > 0.5:\n",
    "                o = 1\n",
    "            else:\n",
    "                if o == 0.5:\n",
    "                    o = 0\n",
    "                else:\n",
    "                    o = -1\n",
    "        print('Somers\\' D score = ', sum(y_score)/len(y_score))\n",
    "\n",
    "def conf_matrix(y, y_score):\n",
    "    y_score = [1 if o >= 0.5 else 0 for o in y_score]\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_score).ravel()\n",
    "    print(\"True negative = \", tn, \", False positive = \", fp, \", False negative = \", fn, \"True positive = \", tp)\n",
    "\n",
    "def metrics_base(y_true, y_score):\n",
    "    somers_d_score(y_true, y_score, True)\n",
    "    y_score = [1 if o >= 0.5 else 0 for o in y_score]\n",
    "    conf_matrix(y_true, y_score)\n",
    "    print('Accuracy score = ', accuracy_score(y_true, y_score))\n",
    "    print('Recall score = ', recall_score(y_true, y_score))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Somers' D scoreeee =  0.6831481481481481\n",
      "True negative =  138 , False positive =  24 , False negative =  32 True positive =  68\n",
      "Accuracy score =  0.7862595419847328\n",
      "Recall score =  0.68\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegressionCV(cv = 5).fit(x_train, y_train)\n",
    "proba_of_default = clf.predict_proba(x_test)[:,1].tolist()\n",
    "y_score = clf.predict(x_test)\n",
    "metrics_base(y_test, proba_of_default)\n",
    "\n",
    "lrcv_hashmap = {}\n",
    "for i in range(len(x_test)):\n",
    "    x = x_test.iloc[i,:]\n",
    "    key = hashlib.sha256(x.values.tobytes()).hexdigest()\n",
    "    lrcv_hashmap[key] = tuple((x, y_score[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_ = tf.placeholder(tf.float32, shape=[None, x_train.shape[1]])\n",
    "output = tf.placeholder(tf.float32, shape=[None,1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "logit = small_network(input_, keep_prob)\n",
    "h = tf.sigmoid(logit)\n",
    "\n",
    "loss = tf.losses.sigmoid_cross_entropy(output, logit)\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.9, beta2=0.9).minimize(loss)\n",
    "\n",
    "base_hashmap = {}\n",
    "\n",
    "def base_classification(data, y, l_r, num_epochs, bsize, print_epoch, test):\n",
    "    num_batches = len(data) // bsize\n",
    "    residue = len(data) % bsize\n",
    "    if residue != 0:\n",
    "        num_batches += 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        h_out = []\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            i = batch * bsize\n",
    "            j = (batch + 1) * bsize\n",
    "            if(residue != 0 and batch + 1 == num_batches):\n",
    "                j = i + residue\n",
    "\n",
    "            if not test:\n",
    "                loss_, h_, _ = sess.run([loss, h, train_op],feed_dict={input_: data.iloc[i:j,:],\n",
    "                                                                       output: y[i:j], lr:l_r, keep_prob:0.5})\n",
    "                h_out.extend([hh[0] for hh in h_])\n",
    "                total_loss += loss_             \n",
    "                \n",
    "            else:\n",
    "                h_ = sess.run(h, feed_dict={input_: data.iloc[i:j,:], keep_prob:0.5})\n",
    "                h_out.extend([hh[0] for hh in h_])\n",
    "\n",
    "                for k in range(i,j):\n",
    "                    x = data.iloc[k,:]\n",
    "                    key = hashlib.sha256(x.values.tobytes()).hexdigest()\n",
    "                    base_hashmap[key] = tuple((x, round(h_out[k])))\n",
    "        \n",
    "        if not test and epoch % print_epoch == 0: \n",
    "            print(\"Epoch {} / {}, Loss = {}\".format(epoch + 1, num_epochs, total_loss / num_batches))\n",
    "\n",
    "    return h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1 / 100, Loss = 0.7743570894565223\n",
      "Epoch 11 / 100, Loss = 0.6572763177583802\n",
      "Epoch 21 / 100, Loss = 0.5915707602815808\n",
      "Epoch 31 / 100, Loss = 0.5465766348928776\n",
      "Epoch 41 / 100, Loss = 0.5158728676022224\n",
      "Epoch 51 / 100, Loss = 0.4953908436703232\n",
      "Epoch 61 / 100, Loss = 0.4811860997721834\n",
      "Epoch 71 / 100, Loss = 0.4710136505792726\n",
      "Epoch 81 / 100, Loss = 0.4635731428861618\n",
      "Epoch 91 / 100, Loss = 0.458206518641058\n",
      "0\n",
      "Somers' D scoreeee =  0.6856259659969088\n",
      "True negative =  580 , False positive =  67 , False negative =  131 True positive =  269\n",
      "Accuracy score =  0.8108882521489972\n",
      "Recall score =  0.6725\n",
      "\n",
      "Testing...\n",
      "0\n",
      "Somers' D scoreeee =  0.6569753086419754\n",
      "True negative =  144 , False positive =  18 , False negative =  37 True positive =  63\n",
      "Accuracy score =  0.7900763358778626\n",
      "Recall score =  0.63\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    base_train_out = base_classification(x_train, y_train, 0.0001, 100, 20, 10, False)\n",
    "    metrics_base(y_train, base_train_out)\n",
    "    \n",
    "    print(\"\\nTesting...\")\n",
    "    base_test_out = base_classification(x_test, y_test, _, 1, 1, _, True) \n",
    "    metrics_base(y_test, base_test_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "num_features = good_train.shape[1]\n",
    "\n",
    "input_left = tf.placeholder(tf.float32, shape=[None, num_features*2])\n",
    "input_right = tf.concat([input_left[:, num_features:], input_left[:, :num_features]], axis=1)\n",
    "output = tf.placeholder(tf.float32, shape=[None,1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "f_left = small_network(input_left, keep_prob)\n",
    "f_right = small_network(input_right, keep_prob)\n",
    "logit = f_left - f_right\n",
    "\n",
    "h = tf.sigmoid(logit)\n",
    "\n",
    "loss = tf.losses.sigmoid_cross_entropy(output, logit)\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.9, beta2=0.999).minimize(loss)\n",
    "\n",
    "symm_hashmap = {}\n",
    "\n",
    "def symmetric_classification(good_data, bad_data, l_r, num_epochs, print_epoch, test): \n",
    "    y = np.empty((len(bad_data) * len(good_data), 1), int)\n",
    "    y.fill(1)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        good_data = shuffle(good_data)\n",
    "        bad_data = shuffle(bad_data)\n",
    "        h_out = []\n",
    "        conc = np.concatenate\n",
    "            \n",
    "        for j in range(len(bad_data)):\n",
    "            \n",
    "            minibatch = []\n",
    "            append = minibatch.append\n",
    "            y_ = np.empty((len(good_data), 1), int)\n",
    "            y_.fill(1.)\n",
    "            \n",
    "            for i in range(len(good_data)):\n",
    "                g = good_data.iloc[i,:].reshape(1,-1)\n",
    "                b = bad_data.iloc[j,:].reshape(1,-1)\n",
    "                append(conc((g, b), axis=None))\n",
    "            \n",
    "                if test:\n",
    "                    hash_good = hashlib.sha256(g.tobytes()).hexdigest()\n",
    "                    hash_bad = hashlib.sha256(b.tobytes()).hexdigest()\n",
    "                    key = tuple((hash_good, hash_bad))\n",
    "                    symm_hashmap[key] = 1\n",
    "           \n",
    "            \n",
    "            if not test:\n",
    "                loss_, h_, _ = sess.run([loss, h, train_op], feed_dict={input_left: minibatch, \n",
    "                                                                    output: y_, lr: l_r, keep_prob:0.75})\n",
    "                h_out.extend([hh[0] for hh in h_])\n",
    "                total_loss += loss_\n",
    "                \n",
    "            else:\n",
    "                h_ = sess.run(h, feed_dict={input_left: minibatch, keep_prob:0.5})\n",
    "                h_out.extend([hh[0] for hh in h_])                        \n",
    "\n",
    "        if not test and epoch % print_epoch == 0: \n",
    "            print(\"Epoch {} / {}, Loss = {}\".format(epoch + 1, num_epochs, total_loss / len(bad_data)))\n",
    "        \n",
    "    return h_out, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1 / 10, Loss = 0.5762374704873543\n",
      "Epoch 2 / 10, Loss = 0.4753151888364618\n",
      "Epoch 3 / 10, Loss = 0.4199894665980818\n",
      "Epoch 4 / 10, Loss = 0.38897737700613055\n",
      "Epoch 5 / 10, Loss = 0.3714740716059112\n",
      "Epoch 6 / 10, Loss = 0.3610201620797654\n",
      "Epoch 7 / 10, Loss = 0.3543860001555735\n",
      "Epoch 8 / 10, Loss = 0.34984017599379813\n",
      "Epoch 9 / 10, Loss = 0.34628695120295105\n",
      "Epoch 10 / 10, Loss = 0.3434892145188459\n",
      "108\n",
      "Somers' D score =  0.7796498306778951\n",
      "True negative =  0 , False positive =  0 , False negative =  39811 True positive =  218989\n",
      "\n",
      "Testing...\n",
      "5\n",
      "Somers' D score =  0.7717987512084253\n",
      "True negative =  0 , False positive =  0 , False negative =  2721 True positive =  13479\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    symm_train_out, y = symmetric_classification(good_train, bad_train, 0.0001, 10, 1, False)\n",
    "    somers_d_score(y, symm_train_out, False)\n",
    "    conf_matrix(y, symm_train_out)\n",
    "    \n",
    "    print(\"\\nTesting...\")\n",
    "    symm_test_out, y = symmetric_classification(good_test, bad_test, _, 1, _, True)\n",
    "    somers_d_score(y, symm_test_out, False)\n",
    "    conf_matrix(y, symm_test_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base & symmetric classification comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukupno preklapanje u procjenama base i symm klasifikatora je:  0.56\n",
      "Ukupno preklapanje u procjenama log.reg. CV i symmetric klasifikatora je:  0.5792592592592593\n"
     ]
    }
   ],
   "source": [
    "N = len(good_test) * len(bad_test)\n",
    "a = 0\n",
    "b=0\n",
    "\n",
    "for i in range(len(good_test)):\n",
    "    for j in range(len(bad_test)):\n",
    "        hash_good = hashlib.sha256(good_test.iloc[i,:].values.tobytes()).hexdigest()\n",
    "        hash_bad = hashlib.sha256(bad_test.iloc[j,:].values.tobytes()).hexdigest()\n",
    "        key = (hash_good, hash_bad)\n",
    "        if((base_hashmap[hash_good][1] == 1 and base_hashmap[hash_bad][1] == 0 and symm_hashmap[key] == 1)\n",
    "          or (base_hashmap[hash_good][1] == 0 and base_hashmap[hash_bad][1] == 1 and symm_hashmap[key] == 0)):\n",
    "            ## oba klasifikatora su jednako zaključila\n",
    "            a += 1\n",
    "        #else:\n",
    "            ## oba klasifikatora nisu zaključila jednako\n",
    "            #print(\"\\n------------------------\\nTestni dobar primjer:\\n\", good_test.iloc[i,:].values, \n",
    "                  #\"\\nBase classificator zaključio je da je ovaj primjer: \", base_hashmap[hash_good][1])\n",
    "            #print(\"\\nTestni loš primjer:\\n\", bad_test.iloc[j,:].values,\n",
    "                  #\"\\nBase classificator zaključio je da je ovaj primjer: \", base_hashmap[hash_bad][1])\n",
    "            #print(\"Symmetric classificator zaključio je da je prvi primjer dobar, a drugi loš: \",symm_hashmap[key])\n",
    "        \n",
    "        if((lrcv_hashmap[hash_good][1] == 1 and lrcv_hashmap[hash_bad][1] == 0 and symm_hashmap[key] == 1)\n",
    "          or (lrcv_hashmap[hash_good][1] == 0 and lrcv_hashmap[hash_bad][1] == 1 and symm_hashmap[key] == 0)):\n",
    "            # lrcv i symm su jednako zaključili\n",
    "            b += 1\n",
    "        #else:\n",
    "            # oba klasifikatora nisu zaključili jednako\n",
    "            #print(\"\\n------------------------\\nTestni dobar primjer:\\n\", good_test.iloc[i,:].values, \n",
    "                  #\"\\nLRCV classificator zaključio je da je ovaj primjer: \", lrcv_hashmap[hash_good][1])\n",
    "            #print(\"\\nTestni loš primjer:\\n\", bad_test.iloc[j,:].values,\n",
    "                  #\"\\nLRCV classificator zaključio je da je ovaj primjer: \", lrcv_hashmap[hash_bad][1])\n",
    "            #print(\"Symmetric classificator zaključio je da je prvi primjer dobar, a drugi loš: \",symm_hashmap[key])\n",
    "            \n",
    "print(\"Ukupno preklapanje u procjenama base i symm klasifikatora je: \", a/N)\n",
    "print(\"Ukupno preklapanje u procjenama log.reg. CV i symmetric klasifikatora je: \", b/N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
