{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import operator, functools, time, itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, recall_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib, time, zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmsc_dataset(file, test):\n",
    "    dataset = pd.read_csv(file)\n",
    "    del dataset['Unnamed: 0']\n",
    "    \n",
    "    if test:\n",
    "        del dataset['SeriousDlqin2yrs']\n",
    "        \n",
    "    for column in dataset.columns.tolist():\n",
    "        imp = Imputer()\n",
    "        dataset[column] = imp.fit_transform(dataset[column].values.reshape(-1,1))\n",
    "    \n",
    "    need_scaling = [e for e in dataset.columns.tolist() if e not in ('SeriousDlqin2yrs', \n",
    "                                                                     'RevolvingUtilizationOfUnsecuredLines',\n",
    "                                                                     'DebtRatio')]\n",
    "    scaler = StandardScaler()\n",
    "    for feature in need_scaling:\n",
    "        dataset[feature] = scaler.fit_transform(dataset[feature].values.reshape(-1,1))\n",
    "                    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_network(input_placeholder, keep_prob):\n",
    "    #reg = tf.contrib.layers.l2_regularizer(scale=0.001)\n",
    "    #layer_1 = tf.layers.dense(input_placeholder, 70, tf.nn.relu, name=\"layer_1\", reuse=tf.AUTO_REUSE,\n",
    "                             #kernel_regularizer = reg)\n",
    "    #drop_out1 = tf.nn.dropout(layer_1, keep_prob)\n",
    "    #layer_2 = tf.layers.dense(layer_1, 60, tf.nn.relu, name=\"layer_2\", reuse=tf.AUTO_REUSE,\n",
    "                             #kernel_regularizer = reg)\n",
    "    #drop_out2 = tf.nn.dropout(layer_2, keep_prob)\n",
    "    #layer_3 = tf.layers.dense(input_placeholder, 40, tf.nn.relu, name=\"layer_3\", reuse=tf.AUTO_REUSE)\n",
    "                             #kernel_regularizer = reg)\n",
    "    #drop_out3 = tf.nn.dropout(layer_3, keep_prob)\n",
    "    layer_4 = tf.layers.dense(input_placeholder, 10, tf.nn.relu, name=\"layer_4\", reuse=tf.AUTO_REUSE)\n",
    "                             #kernel_regularizer = reg)\n",
    "    #drop_out4 = tf.nn.dropout(layer_4, keep_prob)\n",
    "    layer_5 = tf.layers.dense(layer_4, 5, tf.nn.relu, name=\"layer_5\", reuse=tf.AUTO_REUSE)\n",
    "                             #kernel_regularizer = reg)\n",
    "    #drop_out5 = tf.nn.dropout(layer_5, keep_prob)\n",
    "    output = tf.layers.dense(layer_5, 1, name=\"output\", reuse=tf.AUTO_REUSE)\n",
    "                             #kernel_regularizer = reg)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def somers_d_score(y_true, y_score, base):\n",
    "    print(y_score.count(0.5))\n",
    "    if base:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        print('AUC = ', auc_score)\n",
    "        print('Somers\\' D scoreeee = ', 2 * auc_score - 1)\n",
    "    else:\n",
    "        for o in y_score:\n",
    "            if o > 0.5:\n",
    "                o = 1\n",
    "            else:\n",
    "                if o == 0.5:\n",
    "                    o = 0\n",
    "                else:\n",
    "                    o = -1\n",
    "        print('Somers\\' D score = ', sum(y_score)/len(y_score))\n",
    "\n",
    "def metrics_base(y_true, y_score):\n",
    "    somers_d_score(y_true, y_score, True)\n",
    "    y_score = [1 if o >= 0.5 else 0 for o in y_score]\n",
    "    conf_matrix(y_true, y_score)\n",
    "    print('Accuracy score = ', accuracy_score(y_true, y_score))\n",
    "    print('Recall score = ', recall_score(y_true, y_score))  \n",
    "\n",
    "def conf_matrix(y, y_score):\n",
    "    y_score = [1 if o >= 0.5 else 0 for o in y_score]\n",
    "    print(confusion_matrix(y, y_score).ravel())\n",
    "    #tn, fp, fn, tp = confusion_matrix(y, y_score).ravel()\n",
    "    #print(\"True negative = \", tn, \", False positive = \", fp, \", False negative = \", fn, \"True positive = \", tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_classification(data, y, l_r, num_epochs, bsize, print_epoch, test, kaggle):\n",
    "    num_batches = len(data) // bsize\n",
    "    residue = len(data) % bsize\n",
    "    if residue != 0:\n",
    "        num_batches += 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        h_out = []\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            i = batch * bsize\n",
    "            j = (batch + 1) * bsize\n",
    "            if(residue != 0 and batch + 1 == num_batches):\n",
    "                j = i + residue\n",
    "\n",
    "            if not test:\n",
    "                loss_, h_, _ = sess.run([loss, h, train_op],feed_dict={input_: data.iloc[i:j,:],\n",
    "                                                                       output: y[i:j], lr:l_r, keep_prob:0.5})\n",
    "                h_out.extend([hh[0] for hh in h_])\n",
    "                total_loss += loss_             \n",
    "                \n",
    "            else:\n",
    "                h_ = sess.run(h, feed_dict={input_: data.iloc[i:j,:], keep_prob:0.5})\n",
    "                h_out.extend([hh[0] for hh in h_])\n",
    "                \n",
    "                if not kaggle:\n",
    "                    for k in range(i,j):\n",
    "                        x = data.iloc[k,:]\n",
    "                        key = hashlib.sha256(x.values.tobytes()).hexdigest()\n",
    "                        base_hashmap[key] = tuple((x, round(h_out[k])))\n",
    "        \n",
    "        if not test and epoch % print_epoch == 0: \n",
    "            print(\"Epoch {} / {}, Loss = {}\".format(epoch + 1, num_epochs, total_loss / num_batches))\n",
    "\n",
    "    return h_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_classification(good_data, bad_data, l_r, num_epochs, print_epoch, test): \n",
    "    y = np.empty((len(bad_data) * len(good_data), 1), int)\n",
    "    y.fill(1)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        good_data = shuffle(good_data)\n",
    "        bad_data = shuffle(bad_data)\n",
    "        h_out = []\n",
    "        conc = np.concatenate\n",
    "            \n",
    "        for j in range(len(bad_data)):\n",
    "            \n",
    "            minibatch = []\n",
    "            append = minibatch.append\n",
    "            y_ = np.empty((len(good_data), 1), int)\n",
    "            y_.fill(1.)\n",
    "            \n",
    "            for i in range(len(good_data)):\n",
    "                g = good_data.iloc[i,:].reshape(1,-1)\n",
    "                b = bad_data.iloc[j,:].reshape(1,-1)\n",
    "                append(conc((g, b), axis=None))\n",
    "            \n",
    "                if test:\n",
    "                    hash_good = hashlib.sha256(g.tobytes()).hexdigest()\n",
    "                    hash_bad = hashlib.sha256(b.tobytes()).hexdigest()\n",
    "                    key = tuple((hash_good, hash_bad))\n",
    "                    symm_hashmap[key] = 1\n",
    "           \n",
    "            \n",
    "            if not test:\n",
    "                loss_, h_, _ = sess.run([loss, h, train_op], feed_dict={input_left: minibatch, \n",
    "                                                                    output: y_, lr: l_r, keep_prob:0.75})\n",
    "                h_out.extend([hh[0] for hh in h_])\n",
    "                total_loss += loss_\n",
    "                \n",
    "            else:\n",
    "                h_ = sess.run(h, feed_dict={input_left: minibatch, keep_prob:0.5})\n",
    "                h_out.extend([hh[0] for hh in h_])                        \n",
    "\n",
    "        if not test and epoch % print_epoch == 0: \n",
    "            print(\"Epoch {} / {}, Loss = {}\".format(epoch + 1, num_epochs, total_loss / len(bad_data)))\n",
    "        \n",
    "    return h_out, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression CV & Base Classificator Somers' D Comparison\n",
    "Complete dataset is used.\n",
    "The Somers' D metric is calculated from the Kaggle's AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (150000, 11)\n",
      "Default rate:\n",
      " 0.0    139974\n",
      "1.0     10026\n",
      "Name: SeriousDlqin2yrs, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.766127</td>\n",
       "      <td>-0.493860</td>\n",
       "      <td>0.376593</td>\n",
       "      <td>0.802982</td>\n",
       "      <td>0.190194</td>\n",
       "      <td>0.883657</td>\n",
       "      <td>-0.063793</td>\n",
       "      <td>4.409546</td>\n",
       "      <td>-0.057852</td>\n",
       "      <td>1.129387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>-0.832342</td>\n",
       "      <td>-0.100419</td>\n",
       "      <td>0.121876</td>\n",
       "      <td>-0.316001</td>\n",
       "      <td>-0.865297</td>\n",
       "      <td>-0.063793</td>\n",
       "      <td>-0.901283</td>\n",
       "      <td>-0.057852</td>\n",
       "      <td>0.220627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>-0.967735</td>\n",
       "      <td>0.138087</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>-0.281685</td>\n",
       "      <td>-1.253953</td>\n",
       "      <td>0.176056</td>\n",
       "      <td>-0.901283</td>\n",
       "      <td>-0.057852</td>\n",
       "      <td>-0.688133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>-1.509307</td>\n",
       "      <td>-0.100419</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>-0.261655</td>\n",
       "      <td>-0.670969</td>\n",
       "      <td>-0.063793</td>\n",
       "      <td>-0.901283</td>\n",
       "      <td>-0.057852</td>\n",
       "      <td>-0.688133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907239</td>\n",
       "      <td>-0.223074</td>\n",
       "      <td>0.138087</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>4.418944</td>\n",
       "      <td>-0.282312</td>\n",
       "      <td>-0.063793</td>\n",
       "      <td>-0.016145</td>\n",
       "      <td>-0.057852</td>\n",
       "      <td>-0.688133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines       age  \\\n",
       "0               1.0                              0.766127 -0.493860   \n",
       "1               0.0                              0.957151 -0.832342   \n",
       "2               0.0                              0.658180 -0.967735   \n",
       "3               0.0                              0.233810 -1.509307   \n",
       "4               0.0                              0.907239 -0.223074   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse  DebtRatio  MonthlyIncome  \\\n",
       "0                              0.376593   0.802982       0.190194   \n",
       "1                             -0.100419   0.121876      -0.316001   \n",
       "2                              0.138087   0.085113      -0.281685   \n",
       "3                             -0.100419   0.036050      -0.261655   \n",
       "4                              0.138087   0.024926       4.418944   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                         0.883657                -0.063793   \n",
       "1                        -0.865297                -0.063793   \n",
       "2                        -1.253953                 0.176056   \n",
       "3                        -0.670969                -0.063793   \n",
       "4                        -0.282312                -0.063793   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                      4.409546                             -0.057852   \n",
       "1                     -0.901283                             -0.057852   \n",
       "2                     -0.901283                             -0.057852   \n",
       "3                     -0.901283                             -0.057852   \n",
       "4                     -0.016145                             -0.057852   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0            1.129387  \n",
       "1            0.220627  \n",
       "2           -0.688133  \n",
       "3           -0.688133  \n",
       "4           -0.688133  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = gmsc_dataset(\"data\\\\GiveMeSomeCredit\\\\cs-training.csv\", False)\n",
    "dataset_test = gmsc_dataset(\"data\\\\GiveMeSomeCredit\\\\cs-test.csv\", True)\n",
    "print(\"Dataset shape:\", dataset_train.shape)\n",
    "print(\"Default rate:\\n\", dataset_train[\"SeriousDlqin2yrs\"].value_counts())\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 10) (150000, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train = dataset_train['SeriousDlqin2yrs'].reshape(-1,1)\n",
    "del dataset_train['SeriousDlqin2yrs']\n",
    "print(dataset_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression CV - Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegressionCV(cv = 5).fit(dataset_train, y_train)\n",
    "proba_of_default = clf.predict_proba(dataset_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Kaggle submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lrcv = pd.DataFrame(np.column_stack([range(1, len(dataset_test)+1), proba_of_default]), \n",
    "                           columns=['Id', 'Probability'])\n",
    "\n",
    "result_lrcv.Id = result_lrcv.Id.astype(int)\n",
    "result_lrcv.to_csv('result_lrcv.csv', index=False)\n",
    "\n",
    "res_zip = zipfile.ZipFile('result_lrcv.zip', 'w')\n",
    "res_zip.write('result_lrcv.csv', compress_type=zipfile.ZIP_DEFLATED)\n",
    "res_zip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC dobiven predajom rješenja na Kaggle natjecanje je 0.700636.\n",
    "\n",
    "Somers' D je tada 0.401272."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base classification - Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1 / 300, Loss = 4.317015444437663\n",
      "Epoch 51 / 300, Loss = 0.24181966731945673\n",
      "Epoch 101 / 300, Loss = 0.20339563330014546\n",
      "Epoch 151 / 300, Loss = 0.187893130282561\n",
      "Epoch 201 / 300, Loss = 0.18428597311178843\n",
      "Epoch 251 / 300, Loss = 0.18284761627515156\n",
      "0\n",
      "AUC =  0.8559626506226052\n",
      "Somers' D scoreeee =  0.7119253012452105\n",
      "[138594   1380   8221   1805]\n",
      "Accuracy score =  0.9359933333333333\n",
      "Recall score =  0.18003191701575902\n",
      "\n",
      "Testing...\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "input_ = tf.placeholder(tf.float32, shape=[None, dataset_train.shape[1]])\n",
    "output = tf.placeholder(tf.float32, shape=[None,1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "logit = small_network(input_, keep_prob)\n",
    "h = tf.sigmoid(logit)\n",
    "\n",
    "loss = tf.losses.sigmoid_cross_entropy(output, logit)\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print(\"Training...\")\n",
    "    base_train_out = base_classification(dataset_train, y_train, 0.00005, 300, 1000, 50, False, True)\n",
    "    metrics_base(y_train, base_train_out)\n",
    "\n",
    "    print(\"\\nTesting...\")\n",
    "    base_test_out = base_classification(dataset_test, _, 0.00005, 1, 1000, _, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create the Kaggle submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe = pd.DataFrame(np.column_stack([range(1, len(dataset_test)+1), base_test_out]), columns=['Id', 'Probability'])\n",
    "result_dataframe.Id = result_dataframe.Id.astype(int)\n",
    "result_dataframe.to_csv('result_base.csv', index=False)\n",
    "\n",
    "res_zip = zipfile.ZipFile('result_base.zip', 'w')\n",
    "res_zip.write('result_base.csv', compress_type=zipfile.ZIP_DEFLATED)\n",
    "res_zip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC dobiven predajom rješenja na Kaggle natjecanje je 0.852053.\n",
    "\n",
    "Somers' D je tada 0.704106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression, Base and Symmetric classification comparison\n",
    "Data of the first 3 000 clients are used for this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset_train_good shape: (3759, 10)\n",
      "Dataset_train_bad shape: (241, 10)\n",
      "Non-default train rate: 0.93975 , in complete dataset:  0.93316\n",
      "Default train rate: 0.06025 , in complete dataset:  0.06684\n"
     ]
    }
   ],
   "source": [
    "dataset_train = gmsc_dataset(\"data\\\\GiveMeSomeCredit\\\\cs-training.csv\", False)[:4000]\n",
    "\n",
    "dataset_train_good = dataset_train[dataset_train['SeriousDlqin2yrs'] == 0]\n",
    "dataset_train_bad = dataset_train[dataset_train['SeriousDlqin2yrs'] == 1]\n",
    "del dataset_train_good['SeriousDlqin2yrs']\n",
    "del dataset_train_bad['SeriousDlqin2yrs']\n",
    "\n",
    "print(\"Dataset_train_good shape:\", dataset_train_good.shape)\n",
    "print(\"Dataset_train_bad shape:\", dataset_train_bad.shape)\n",
    "\n",
    "good_train_rate = len(dataset_train_good)/4000\n",
    "bad_train_rate = len(dataset_train_bad)/4000\n",
    "print(\"Non-default train rate:\", good_train_rate, \", in complete dataset: \", 139974/150000)\n",
    "print(\"Default train rate:\", bad_train_rate, \", in complete dataset: \", 10026/150000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes of datasets for symmetric classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions for symmetric classifier: (3007, 10) (752, 10) (192, 10) (49, 10)\n"
     ]
    }
   ],
   "source": [
    "good_train, good_test = train_test_split(dataset_train_good, test_size=0.2)\n",
    "bad_train, bad_test = train_test_split(dataset_train_bad, test_size=0.2)\n",
    "print(\"\\nDimensions for symmetric classifier:\", good_train.shape, good_test.shape, bad_train.shape, bad_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes of datasets for base classification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions for base classifier: (3199, 10) (3199, 1) (801, 10) (801, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array([1]*len(good_train) + [0]*len(bad_train)).reshape(-1,1)\n",
    "x_train = good_train.append(bad_train)\n",
    "x_train['SeriousDlqin2yrs'] = y_train\n",
    "x_train = shuffle(x_train)\n",
    "y_train = x_train['SeriousDlqin2yrs'].reshape(-1,1)\n",
    "del x_train['SeriousDlqin2yrs']\n",
    "\n",
    "y_test = np.array([1]*len(good_test) + [0]*len(bad_test)).reshape(-1,1)\n",
    "x_test = good_test.append(bad_test)\n",
    "x_test['SeriousDlqin2yrs'] = y_test\n",
    "x_test = shuffle(x_test)\n",
    "y_test = x_test['SeriousDlqin2yrs'].reshape(-1,1)\n",
    "del x_test['SeriousDlqin2yrs']\n",
    "\n",
    "print(\"\\nDimensions for base classifier:\", x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "AUC =  0.6697514112027789\n",
      "Somers' D scoreeee =  0.33950282240555785\n",
      "[  0  49   1 751]\n",
      "Accuracy score =  0.9375780274656679\n",
      "Recall score =  0.9986702127659575\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegressionCV(cv = 5).fit(x_train, y_train)\n",
    "proba_of_default = clf.predict_proba(x_test)[:,1].tolist()\n",
    "y_score = clf.predict(x_test)\n",
    "metrics_base(y_test, proba_of_default)\n",
    "\n",
    "lrcv_hashmap = {}\n",
    "for i in range(len(x_test)):\n",
    "    x = x_test.iloc[i,:]\n",
    "    key = hashlib.sha256(x.values.tobytes()).hexdigest()\n",
    "    lrcv_hashmap[key] = tuple((x, y_score[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training base...\n",
      "Epoch 1 / 4000, Loss = 0.8652895047114446\n",
      "Epoch 301 / 4000, Loss = 0.16233316980875456\n",
      "Epoch 601 / 4000, Loss = 0.14940325686564812\n",
      "Epoch 901 / 4000, Loss = 0.14585440319318038\n",
      "Epoch 1201 / 4000, Loss = 0.14502553011362368\n",
      "Epoch 1501 / 4000, Loss = 0.13990527391433716\n",
      "Epoch 1801 / 4000, Loss = 0.1378672426709762\n",
      "Epoch 2101 / 4000, Loss = 0.14468433192143074\n",
      "Epoch 2401 / 4000, Loss = 0.13967442913697317\n",
      "Epoch 2701 / 4000, Loss = 0.13904799062472123\n",
      "Epoch 3001 / 4000, Loss = 0.14111473927131066\n",
      "Epoch 3301 / 4000, Loss = 0.1401433635216493\n",
      "Epoch 3601 / 4000, Loss = 0.13863081771593827\n",
      "Epoch 3901 / 4000, Loss = 0.1381235094024585\n",
      "0\n",
      "AUC =  0.894710086187784\n",
      "Somers' D scoreeee =  0.789420172375568\n",
      "[  76  116   18 2989]\n",
      "Accuracy score =  0.9581119099718662\n",
      "Recall score =  0.9940139674093781\n",
      "\n",
      "Testing base...\n",
      "0\n",
      "AUC =  0.7210703430308294\n",
      "Somers' D scoreeee =  0.4421406860616588\n",
      "[  8  41   8 744]\n",
      "Accuracy score =  0.9388264669163545\n",
      "Recall score =  0.9893617021276596\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "input_ = tf.placeholder(tf.float32, shape=[None, x_train.shape[1]])\n",
    "output = tf.placeholder(tf.float32, shape=[None,1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "logit = small_network(input_, keep_prob)\n",
    "h = tf.sigmoid(logit)\n",
    "\n",
    "loss = tf.losses.sigmoid_cross_entropy(output, logit) #+ tf.losses.get_regularization_loss()\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "base_hashmap = {}\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "print(\"Training base...\")\n",
    "base_train_out = base_classification(x_train, y_train, 0.005, 4000, 250, 300, False, False)\n",
    "metrics_base(y_train, base_train_out)\n",
    "\n",
    "print(\"\\nTesting base...\")\n",
    "base_test_out = base_classification(x_test, y_test, _, 1, 250, _, True, False)\n",
    "metrics_base(y_test, base_test_out)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training symmetric...\n",
      "Epoch 1 / 20, Loss = 17.647120827808976\n",
      "Epoch 2 / 20, Loss = 2.541292779283443\n",
      "Epoch 3 / 20, Loss = 2.4939688363174355\n",
      "Epoch 4 / 20, Loss = 1.6394102160969244\n",
      "Epoch 5 / 20, Loss = 1.570183419532744\n",
      "Epoch 6 / 20, Loss = 1.9399662270831566\n",
      "Epoch 7 / 20, Loss = 1.1764947825249692\n",
      "Epoch 8 / 20, Loss = 1.2153589203662705\n",
      "Epoch 9 / 20, Loss = 1.628168617652591\n",
      "Epoch 10 / 20, Loss = 1.6255313891742844\n",
      "Epoch 11 / 20, Loss = 1.093409926600998\n",
      "Epoch 12 / 20, Loss = 1.4204267230428134\n",
      "Epoch 13 / 20, Loss = 1.030693696514561\n",
      "Epoch 14 / 20, Loss = 1.649463577092386\n",
      "Epoch 15 / 20, Loss = 1.3863756124580202\n",
      "Epoch 16 / 20, Loss = 1.5164884295653185\n",
      "Epoch 17 / 20, Loss = 1.1953082637963537\n",
      "Epoch 18 / 20, Loss = 0.8134677523339633\n",
      "Epoch 19 / 20, Loss = 1.107283457473386\n",
      "Epoch 20 / 20, Loss = 1.5837496424404283\n",
      "0\n",
      "Somers' D score =  0.6567305701338398\n",
      "[     0      0 142515 434829]\n",
      "\n",
      "Testing symmetric...\n",
      "0\n",
      "Somers' D score =  0.6760922406887734\n",
      "[    0     0  8504 28344]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "num_features = good_train.shape[1]\n",
    "\n",
    "input_left = tf.placeholder(tf.float32, shape=[None, num_features*2])\n",
    "input_right = tf.concat([input_left[:, num_features:], input_left[:, :num_features]], axis=1)\n",
    "output = tf.placeholder(tf.float32, shape=[None,1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "f_left = small_network(input_left, keep_prob)\n",
    "f_right = small_network(input_right, keep_prob)\n",
    "logit = f_left - f_right\n",
    "\n",
    "h = tf.sigmoid(logit)\n",
    "\n",
    "loss = tf.losses.sigmoid_cross_entropy(output, logit)\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.9, beta2=0.999).minimize(loss)\n",
    "\n",
    "symm_hashmap = {}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"Training symmetric...\")\n",
    "    symm_train_out, y = symmetric_classification(good_train, bad_train, 0.001, 20, 1, False)\n",
    "    somers_d_score(y, symm_train_out, False)\n",
    "    conf_matrix(y, symm_train_out)\n",
    "    \n",
    "    print(\"\\nTesting symmetric...\")\n",
    "    symm_test_out, y = symmetric_classification(good_test, bad_test, _, 1, _, True)\n",
    "    somers_d_score(y, symm_test_out, False)\n",
    "    conf_matrix(y, symm_test_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRCV, Base & Symmetric Classification Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukupno preklapanje u procjenama base i symm klasifikatora je:  0.16152844116369952\n",
      "Ukupno preklapanje u procjenama log.reg. CV i symmetric klasifikatora je:  0.0\n"
     ]
    }
   ],
   "source": [
    "N = len(good_test) * len(bad_test)\n",
    "a = 0\n",
    "b = 0\n",
    "\n",
    "for i in range(len(good_test)):\n",
    "    for j in range(len(bad_test)):\n",
    "        hash_good = hashlib.sha256(good_test.iloc[i,:].values.tobytes()).hexdigest()\n",
    "        hash_bad = hashlib.sha256(bad_test.iloc[j,:].values.tobytes()).hexdigest()\n",
    "        key = (hash_good, hash_bad)\n",
    "        if((base_hashmap[hash_good][1] == 1 and base_hashmap[hash_bad][1] == 0 and symm_hashmap[key] == 1)\n",
    "          or (base_hashmap[hash_good][1] == 0 and base_hashmap[hash_bad][1] == 1 and symm_hashmap[key] == 0)):\n",
    "            ## oba klasifikatora su jednako zaključila\n",
    "            a += 1\n",
    "        #else:\n",
    "            ## oba klasifikatora nisu zaključila jednako\n",
    "            #print(\"\\n------------------------\\nTestni dobar primjer:\\n\", good_test.iloc[i,:].values, \n",
    "                  #\"\\nBase classificator zaključio je da je ovaj primjer: \", base_hashmap[hash_good][1])\n",
    "            #print(\"\\nTestni loš primjer:\\n\", bad_test.iloc[j,:].values,\n",
    "                  #\"\\nBase classificator zaključio je da je ovaj primjer: \", base_hashmap[hash_bad][1])\n",
    "            #print(\"Symmetric classificator zaključio je da je prvi primjer dobar, a drugi loš: \",symm_hashmap[key])\n",
    "        \n",
    "        if((lrcv_hashmap[hash_good][1] == 1 and lrcv_hashmap[hash_bad][1] == 0 and symm_hashmap[key] == 1)\n",
    "          or (lrcv_hashmap[hash_good][1] == 0 and lrcv_hashmap[hash_bad][1] == 1 and symm_hashmap[key] == 0)):\n",
    "            # lrcv i symm su jednako zaključili\n",
    "            b += 1\n",
    "        #else:\n",
    "            # oba klasifikatora nisu zaključili jednako\n",
    "            #print(\"\\n------------------------\\nTestni dobar primjer:\\n\", good_test.iloc[i,:].values, \n",
    "                  #\"\\nLRCV classificator zaključio je da je ovaj primjer: \", lrcv_hashmap[hash_good][1])\n",
    "            #print(\"\\nTestni loš primjer:\\n\", bad_test.iloc[j,:].values,\n",
    "                  #\"\\nLRCV classificator zaključio je da je ovaj primjer: \", lrcv_hashmap[hash_bad][1])\n",
    "            #print(\"Symmetric classificator zaključio je da je prvi primjer dobar, a drugi loš: \",symm_hashmap[key])\n",
    "            \n",
    "print(\"Ukupno preklapanje u procjenama base i symm klasifikatora je: \", a/N)\n",
    "print(\"Ukupno preklapanje u procjenama log.reg. CV i symmetric klasifikatora je: \", b/N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
